{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install faiss-gpu\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install translate\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install langdetect\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhattuong/anaconda3/envs/tuong/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "import translate\n",
    "\n",
    "class Translation:\n",
    "    def __init__(self, from_lang='vi', to_lang='en', mode='google'):\n",
    "        # The class Translation is a wrapper for the two translation libraries, googletrans and translate. \n",
    "        self.__mode = mode\n",
    "        self.__from_lang = from_lang\n",
    "        self.__to_lang = to_lang\n",
    "\n",
    "        if mode in 'googletrans':\n",
    "            self.translator = googletrans.Translator()\n",
    "        elif mode in 'translate':\n",
    "            self.translator = translate.Translator(from_lang=from_lang,to_lang=to_lang)\n",
    "\n",
    "    def preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        It takes a string as input, and returns a string with all the letters in lowercase\n",
    "        :param text: The text to be processed\n",
    "        :return: The text is being returned in lowercase.\n",
    "        \"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        The function takes in a text and preprocesses it before translation\n",
    "        :param text: The text to be translated\n",
    "        :return: The translated text.\n",
    "        \"\"\"\n",
    "        text = self.preprocessing(text)\n",
    "        return self.translator.translate(text) if self.__mode in 'translate' \\\n",
    "                else self.translator.translate(text, dest=self.__to_lang).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File4Faiss:\n",
    "  def __init__(self, root_database: str):\n",
    "    self.root_database = root_database\n",
    "\n",
    "  def re_shot_list(self, shot_list, id, k):\n",
    "    len_lst = len(shot_list)\n",
    "    if k>=len_lst or k == 0:\n",
    "      return shot_list\n",
    "\n",
    "    shot_list.sort()\n",
    "    index_a = shot_list.index(id)\n",
    "\n",
    "    index_get_right = k // 2\n",
    "    index_get_left = k - index_get_right\n",
    "\n",
    "    if index_a - index_get_left < 0:\n",
    "      index_get_left = index_a\n",
    "      index_get_right = k - index_a\n",
    "    elif index_a + index_get_right >= len_lst:\n",
    "      index_get_right = len_lst - index_a - 1\n",
    "      index_get_left = k - index_get_right\n",
    "\n",
    "    output = shot_list[index_a - index_get_left: index_a] + shot_list[index_a: index_a + index_get_right + 1]\n",
    "    return output\n",
    "\n",
    "  def write_json_file(self, json_path: str, shot_frames_path: str, option='full'):\n",
    "    count = 0\n",
    "    self.infos = []\n",
    "    des_path = os.path.join(json_path, \"keyframes_id.json\")\n",
    "    keyframe_paths = sorted(glob.glob(f'{self.root_database}/KeyFramesC0*'))\n",
    "\n",
    "    for kf in keyframe_paths:\n",
    "      video_paths = sorted(glob.glob(f\"{kf}/*\"))\n",
    "\n",
    "      for video_path in video_paths:\n",
    "        image_paths = sorted(glob.glob(f'{video_path}/*.jpg'))\n",
    "\n",
    "        ###### Get all id keyframes from video_path ######\n",
    "        id_keyframes = np.array([int(id.split('/')[-1].replace('.jpg', '')) for id in image_paths])\n",
    "\n",
    "        ###### Get scenes from video_path ######\n",
    "        video_info = video_path.split('/')[-1]\n",
    "        \n",
    "        with open(f'{shot_frames_path}/{video_info}.txt', 'r') as f:\n",
    "          lst_range_shotes = f.readlines()\n",
    "        lst_range_shotes = np.array([re.sub('\\[|\\]', '', line).strip().split(' ') for line in lst_range_shotes]).astype(np.uint32)\n",
    "\n",
    "        for im_path in image_paths:\n",
    "          # im_path = 'Database/' + '/'.join(im_path.split('/')[-3:])\n",
    "          id = int(im_path.split('/')[-1].replace('.jpg', ''))\n",
    "          \n",
    "          i = 0\n",
    "          flag=0\n",
    "          for range_shot in lst_range_shotes:\n",
    "            i+=1\n",
    "            first, end = range_shot\n",
    "\n",
    "            if first <= id <= end:\n",
    "              break\n",
    "            \n",
    "            if i == len(lst_range_shotes):\n",
    "              flag=1\n",
    "          \n",
    "          if flag == 1:\n",
    "            print(f\"Skip: {im_path}\")\n",
    "            print(first, end)\n",
    "            continue\n",
    "\n",
    "          ##### Get List Shot ID #####\n",
    "          lst_shot = id_keyframes[np.where((id_keyframes>=first) & (id_keyframes<=end))]\n",
    "          lst_shot = self.re_shot_list(list(lst_shot), id, k=6)\n",
    "          lst_shot = [f\"{i:0>6d}\" for i in lst_shot]\n",
    "\n",
    "          ##### Get List Shot Path #####\n",
    "          lst_shot_path = []\n",
    "          for id_shot in lst_shot:\n",
    "            info_shot = {\n",
    "                \"shot_id\": id_shot,\n",
    "                \"shot_path\": '/'.join(im_path.split('/')[:-1]) + f\"/{id_shot}.jpg\"\n",
    "            }\n",
    "            lst_shot_path.append(info_shot) \n",
    "\n",
    "          ##### Merge All Info #####\n",
    "          info = {\n",
    "                  \"image_path\": im_path,\n",
    "                  \"list_shot_id\": lst_shot,\n",
    "                  \"list_shot_path\": lst_shot_path\n",
    "                 }\n",
    "                  \n",
    "          if option == 'full':        \n",
    "            self.infos.append(info)   \n",
    "          else:\n",
    "            if id == (end+first)//2:\n",
    "              self.infos.append(info)  \n",
    "\n",
    "          count += 1\n",
    "\n",
    "    id2img_fps = dict(enumerate(self.infos))\n",
    "    \n",
    "    with open(des_path, 'w') as f:\n",
    "      f.write(json.dumps(id2img_fps))\n",
    "\n",
    "    print(f'Saved {des_path}')\n",
    "    print(f\"Number of Index: {count}\")\n",
    "\n",
    "  def load_json_file(self, json_path: str):\n",
    "    with open(json_path, 'r') as f:\n",
    "      js = json.loads(f.read())\n",
    "\n",
    "    return {int(k):v for k,v in js.items()}\n",
    "\n",
    "  def write_bin_file(self, bin_path: str, json_path: str, method='L2', feature_shape=512):\n",
    "    count = 0\n",
    "    id2img_fps = self.load_json_file(json_path)\n",
    "\n",
    "    if method in 'L2':\n",
    "      index = faiss.IndexFlatL2(feature_shape)\n",
    "    elif method in 'cosine':\n",
    "      index = faiss.IndexFlatIP(feature_shape)\n",
    "    else:\n",
    "      assert f\"{method} not supported\"\n",
    "    \n",
    "    for _, value in id2img_fps.items():\n",
    "      image_path = value[\"image_path\"]\n",
    "      video_name = image_path.split('/')[-2] + '.npy'\n",
    "\n",
    "      video_id = re.sub('_V\\d+', '', image_path.split('/')[-2])\n",
    "      batch_name = image_path.split('/')[-3].split('_')[-1]\n",
    "      clip_name = f\"CLIPFeatures_{video_id}_{batch_name}\"\n",
    "\n",
    "      feat_path = os.path.join(self.root_database, clip_name, video_name) \n",
    "\n",
    "      feats = np.load(feat_path)\n",
    "\n",
    "      ids = os.listdir(re.sub('/\\d+.jpg','',image_path))\n",
    "      ids = sorted(ids, key=lambda x:int(x.split('.')[0]))\n",
    "\n",
    "      id = ids.index(image_path.split('/')[-1])\n",
    "      \n",
    "      feat = feats[id]\n",
    "      feat = feat.astype(np.float32).reshape(1,-1)\n",
    "      index.add(feat)\n",
    "      \n",
    "      count += 1\n",
    "    \n",
    "    faiss.write_index(index, os.path.join(bin_path, f\"faiss_{method}.bin\"))\n",
    "\n",
    "    print(f'Saved {os.path.join(bin_path, f\"faiss_{method}.bin\")}')\n",
    "    print(f\"Number of Index: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file = File4Faiss('/home/hoangtv/Desktop/disk_1TB/Tuong/AIC_2022/Database')\n",
    "create_file.write_json_file(json_path='/content/drive/MyDrive/Video_Retrieval/faiss_merge_files', shot_frames_path='/content/drive/MyDrive/Database/scenes_txt')\n",
    "create_file.write_bin_file(bin_path='/content/drive/MyDrive/Video_Retrieval/faiss_merge_files', json_path='/content/drive/MyDrive/Video_Retrieval/faiss_merge_files/keyframes_id.json', method='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFaiss:\n",
    "  def __init__(self, root_database: str, bin_file: str, json_path: str):    \n",
    "    self.index = self.load_bin_file(bin_file)\n",
    "    self.id2img_fps = self.load_json_file(json_path)\n",
    "\n",
    "    self.translater = Translation()\n",
    "    \n",
    "    self.__device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.model, preprocess = clip.load(\"ViT-B/16\", device=self.__device)\n",
    "    \n",
    "  def load_json_file(self, json_path: str):\n",
    "      with open(json_path, 'r') as f:\n",
    "        js = json.loads(f.read())\n",
    "\n",
    "      return {int(k):v for k,v in js.items()}\n",
    "\n",
    "  def load_bin_file(self, bin_file: str):\n",
    "    return faiss.read_index(bin_file)\n",
    "\n",
    "  def show_images(self, image_paths):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    columns = int(math.sqrt(len(image_paths)))\n",
    "    rows = int(np.ceil(len(image_paths)/columns))\n",
    "\n",
    "    for i in range(1, columns*rows +1):\n",
    "      img = plt.imread(image_paths[i - 1])\n",
    "      ax = fig.add_subplot(rows, columns, i)\n",
    "      ax.set_title('/'.join(image_paths[i - 1].split('/')[-3:]))\n",
    "\n",
    "      plt.imshow(img)\n",
    "      plt.axis(\"off\")\n",
    "      \n",
    "    plt.show()\n",
    "\n",
    "  def image_search(self, id_query, k):    \n",
    "    query_feats = self.index.reconstruct(id_query).reshape(1,-1)\n",
    "\n",
    "    scores, idx_image = self.index.search(query_feats, k=k)\n",
    "    idx_image = idx_image.flatten()\n",
    "\n",
    "    infos_query = list(map(self.id2img_fps.get, list(idx_image)))\n",
    "    image_paths = [info['image_path'] for info in infos_query]\n",
    "    \n",
    "    # print(f\"scores: {scores}\")\n",
    "    # print(f\"idx: {idx_image}\")\n",
    "    # print(f\"paths: {image_paths}\")\n",
    "    \n",
    "    return scores, idx_image, infos_query, image_paths\n",
    "\n",
    "  def text_search(self, text, k):\n",
    "    if detect(text) == 'vi':\n",
    "      text = self.translater(text)\n",
    "\n",
    "    ###### TEXT FEATURES EXACTING ######\n",
    "    text = clip.tokenize([text]).to(self.__device)  \n",
    "    text_features = self.model.encode_text(text).cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "    ###### SEARCHING #####\n",
    "    scores, idx_image = self.index.search(text_features, k=k)\n",
    "    idx_image = idx_image.flatten()\n",
    "\n",
    "    ###### GET INFOS KEYFRAMES_ID ######\n",
    "    infos_query = list(map(self.id2img_fps.get, list(idx_image)))\n",
    "    image_paths = [info['image_path'] for info in infos_query]\n",
    "    # lst_shot = [info['list_shot_id'] for info in infos_query]\n",
    "\n",
    "    # print(f\"scores: {scores}\")\n",
    "    # print(f\"idx: {idx_image}\")\n",
    "    # print(f\"paths: {image_paths}\")\n",
    "\n",
    "    return scores, idx_image, infos_query, image_paths\n",
    "\n",
    "  def write_csv(self, infos_query, des_path):\n",
    "    check_files = []\n",
    "    \n",
    "    ### GET INFOS SUBMIT ###\n",
    "    for info in infos_query:\n",
    "      video_name = info['image_path'].split('/')[-2]\n",
    "      lst_frames = info['list_shot_id']\n",
    "\n",
    "      for id_frame in lst_frames:\n",
    "        check_files.append(os.path.join(video_name, id_frame))\n",
    "    ###########################\n",
    "    \n",
    "    check_files = set(check_files)\n",
    "\n",
    "    if os.path.exists(des_path):\n",
    "        df_exist = pd.read_csv(des_path, header=None)\n",
    "        lst_check_exist = df_exist.values.tolist()      \n",
    "        check_exist = [info[0].replace('.mp4','/') + f\"{info[1]:0>6d}\" for info in lst_check_exist]\n",
    "\n",
    "        ##### FILTER EXIST LINES FROM SUBMIT.CSV FILE #####\n",
    "        check_files = [info for info in check_files if info not in check_exist]\n",
    "    else:\n",
    "      check_exist = []\n",
    "\n",
    "    video_names = [i.split('/')[0] + '.mp4' for i in check_files]\n",
    "    frame_ids = [i.split('/')[-1] for i in check_files]\n",
    "\n",
    "    dct = {'video_names': video_names, 'frame_ids': frame_ids}\n",
    "    df = pd.DataFrame(dct)\n",
    "\n",
    "    if len(check_files) + len(check_exist) < 99:\n",
    "      df.to_csv(des_path, mode='a', header=False, index=False)\n",
    "      print(f\"Save submit file to {des_path}\")\n",
    "    else:\n",
    "      print('Exceed the allowed number of lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TESTING #####\n",
    "bin_file='/content/drive/MyDrive/Video_Retrieval/faiss_for_database/faiss_cosine.bin'\n",
    "json_path = '/content/drive/MyDrive/Video_Retrieval/faiss_for_database/keyframes_id.json'\n",
    "\n",
    "cosine_faiss = MyFaiss('/content/drive/MyDrive/Merge_Database', bin_file, json_path)\n",
    "\n",
    "##### IMAGE SEARCH #####\n",
    "# i_scores, _, infos_query, i_image_paths = cosine_faiss.image_search(id_query=9999, k=9)\n",
    "# cosine_faiss.write_csv(infos_query, des_path='/content/submit.csv')\n",
    "# cosine_faiss.show_images(i_image_paths)\n",
    "\n",
    "##### TEXT SEARCH #####\n",
    "text = 'Người nghệ nhân đang tô màu cho chiếc mặt nạ một cách tỉ mỉ. \\\n",
    "      Xung quanh ông là rất nhiều những chiếc mặt nạ. \\\n",
    "      Người nghệ nhân đi đôi dép tổ ong rất giản dị. \\\n",
    "      Sau đó là hình ảnh quay cận những chiếc mặt nạ. \\\n",
    "      Loại mặt nạ này được gọi là mặt nạ giấy bồi Trung thu.'\n",
    "\n",
    "scores, _, infos_query, image_paths = cosine_faiss.text_search(text, k=9)\n",
    "cosine_faiss.write_csv(infos_query, des_path='/content/submit.csv')\n",
    "# cosine_faiss.show_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  ##### CREATE JSON AND BIN FILES #####\n",
    "  # create_file = File4Faiss('./Database')\n",
    "  # create_file.write_json_file(json_path='./', shot_frames_path='./scenes_txt')\n",
    "  # create_file.write_bin_file(bin_path='./', json_path='./keyframes_id.json', method='cosine')\n",
    "\n",
    "  ##### TESTING #####\n",
    "  bin_file='./faiss_cosine.bin'\n",
    "  json_path = './dict/keyframes_id.json'\n",
    "\n",
    "  cosine_faiss = MyFaiss('./Database', bin_file, json_path)\n",
    "\n",
    "  ##### IMAGE SEARCH #####\n",
    "  i_scores, infos_query, i_image_paths = cosine_faiss.text_search(id_query=9999, k=9)\n",
    "  # cosine_faiss.write_csv(infos_query, des_path_submit='./')\n",
    "  cosine_faiss.show_images(i_image_paths)\n",
    "\n",
    "  ##### TEXT SEARCH #####\n",
    "  text = 'Người nghệ nhân đang tô màu cho chiếc mặt nạ một cách tỉ mỉ. \\\n",
    "        Xung quanh ông là rất nhiều những chiếc mặt nạ. \\\n",
    "        Người nghệ nhân đi đôi dép tổ ong rất giản dị. \\\n",
    "        Sau đó là hình ảnh quay cận những chiếc mặt nạ. \\\n",
    "        Loại mặt nạ này được gọi là mặt nạ giấy bồi Trung thu.'\n",
    "\n",
    "  scores, infos_query, image_paths = cosine_faiss.text_search(text, k=9)\n",
    "  # cosine_faiss.write_csv(infos_query, des_path_submit='./')\n",
    "  cosine_faiss.show_images(image_paths)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c040059c337deb504f19c673fdcf9a2751b584394b1f9883eb09580a791bf0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
